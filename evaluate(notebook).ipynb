{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy_of_Copy_of_Prior_DQN_agent (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-CKZPR60AVQ"
      },
      "source": [
        "Mounts notebook to drive ir order to have access to several needed files \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVKM5urXztt9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VImHDH9s0wIC"
      },
      "source": [
        "Import the Necessary Packages and define device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIqPKyFow8FE"
      },
      "source": [
        "%cd /content/drive/My Drive/codes/\n",
        "import gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import os.path as osp\n",
        "from atari_wrappers import wrap_deepmind\n",
        "from agent_train import Agent\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb6JiRTn0h32"
      },
      "source": [
        "Create environment according to deepmind's preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgiQqXd5xvU9"
      },
      "source": [
        "env = gym.make('MsPacmanNoFrameskip-v0')\n",
        "env = wrap_deepmind(env)\n",
        "print('State shape: ', env.observation_space.shape)\n",
        "print('Number of actions: ', env.action_space.n)\n",
        "state_shape=env.observation_space.shape\n",
        "act=env.action_space.n\n",
        "state = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLcuL4P11Hur"
      },
      "source": [
        "Set specific value for random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqZ1DMZpxqgb"
      },
      "source": [
        "#seed\n",
        "RANDOM_SEED = 379\n",
        "env.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "env.action_space.seed(RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI1ICvZ41GSF"
      },
      "source": [
        "Create agent according to desirable algorithm \n",
        "Moreover set hyperparameters memory size, gamma, lr, update frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMj7vJ_CVISu"
      },
      "source": [
        "agent=Agent(state_shape,act,RANDOM_SEED,memory_size=500000,gamma=0.99,lr=1.5e-4,update_frequency=4,beta=0,norm=False,double=True,duel=False,noisy=False)\n",
        "print(agent.optimizer,agent.steps)\n",
        "print(len(agent.memory))\n",
        "agent.net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHSdQ9A361Sj"
      },
      "source": [
        "Training phase of the agent for specific training_steps with\n",
        " hyperparameters:eps_sart,eps_end,eps_decay,target_update,\n",
        "\n",
        "*   eps_sart, eps_end, eps_decay\n",
        "*   min_frames, max_frames, batch_size, target_update\n",
        "\n",
        "Additionally save net weights in selected location and track metrics for tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbBPo1-rI82x"
      },
      "source": [
        "eps_start=1.0\n",
        "eps_end=0.01\n",
        "eps_decay=1000000\n",
        "beta_start = 0.4\n",
        "beta_decay=10000000\n",
        "beta_end=1.0\n",
        "target_update = 10000\n",
        "save_net_weights=200000\n",
        "steps_done= 0\n",
        "start=time.time()\n",
        "start1=start\n",
        "start2=start\n",
        "max_frames=105000\n",
        "min_frames = 50000\n",
        "batch_size=32\n",
        "tot_rew= []\n",
        "start=time.time()\n",
        "steps_prev = 0\n",
        "writer = SummaryWriter()\n",
        "while steps_done<10000000:\n",
        "    loss=0\n",
        "    state = env.reset()\n",
        "    score=0\n",
        "    for j in range(max_frames):\n",
        "        eps = max(eps_end, eps_start -steps_done / eps_decay)    \n",
        "        agent.beta = min(beta_end, beta_start + steps_done * (1.0 - beta_start) / beta_decay)\n",
        "        steps_done += 1\n",
        "        oldstate=state\n",
        "        action=agent.select_action(oldstate,eps)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        score += reward\n",
        "        loss=agent.step(oldstate,action,reward,state,done,batch_size,min_frames) \n",
        "        if (steps_done % target_update == 0):\n",
        "            agent.t_net.load_state_dict(agent.net.state_dict()) \n",
        "            if (steps_done % save_net_weights==0):\n",
        "              s1=\"/content/drive/My Drive/thesis/classic_agent/prior_double_DQN_agent_v0\"\n",
        "              s2=str(int(steps_done/100000))\n",
        "              s3=\".pth\"\n",
        "              torch.save(agent.net.state_dict(), s1+s2+s3)\n",
        "        if done:\n",
        "            tot_rew.append(score)\n",
        "            writer.add_scalar(\"loss\", loss, steps_done)\n",
        "            if(len(tot_rew)%200==0):\n",
        "              speed=(steps_done-steps_prev)/(time.time()-start)\n",
        "              start=time.time()\n",
        "              steps_prev=steps_done\n",
        "              mean_rew=np.mean(tot_rew[-100:])\n",
        "              writer.add_scalar(\"epsilon\", eps, steps_done)\n",
        "              writer.add_scalar(\"speed\", speed, steps_done)\n",
        "              writer.add_scalar(\"reward_100\", mean_rew, steps_done)\n",
        "              writer.add_scalar(\"score\", score, steps_done)\n",
        "              print(\"%d:done %d episodes %.3f mean_rew  %.2f fps\" %(steps_done,len(tot_rew),mean_rew,speed))\n",
        "            break \n",
        "    env.close()\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}